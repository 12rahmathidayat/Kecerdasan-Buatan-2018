{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nama : M. Taufik\n",
    "\n",
    "# Nim : 09011281823073\n",
    "\n",
    "# Kelas : SK 5 B Inderalaya\n",
    "\n",
    "#                             TUGAS UTS KECERDASAN BUATAN\n",
    "\n",
    "# Gender Prediction with Logistic Regression and ANN\n",
    "\n",
    "# Source : https://www.kaggle.com/erdemuysal/gender-prediction-with-logistic-regression-and-ann\n",
    "\n",
    "Pada tugas ini saya memakai data berupa Memprediksi Jenis Kelamin dengan logika Regresi dan Artificial Neural Network. Dari dataset ini menghasilkan output akhir berupa pengklasifikasian jenis kelamin memakai Logika regresi dan Jaringan syaraf tiruan\n",
    "\n",
    "Adapun Pengertian regresi dalam statistika adalah salah satu metode untuk menentukan hubungan sebab-akibat antara satu variabel dengan variabel(-variabel) yang lain. Variabel \"penyebab\" disebut dengan bermacam-macam istilah: variabel penjelas, variabel eksplanatorik, variabel independen, atau secara bebas, variabel X (karena sering kali digambarkan dalam grafik sebagai absis, atau sumbu X). Variabel terkena akibat dikenal sebagai variabel yang dipengaruhi, variabel dependen, variabel terikat, atau variabel Y. Kedua variabel ini dapat merupakan variabel acak (random), namun variabel yang dipengaruhi harus selalu variabel acak. \n",
    "\n",
    "Cara kerja Neural Network dapat dianalogikan sebagaiman halnya manusia belajar dengan mengunakan contoh atau yang disebut sebagai supervised learning. Sebuah Neural Network dikonfigurasi untuk aplikasi tertentu, seperti pengenalan pola atau klasifikasi data, dan kemudian disempurnakan melalui proses pembelajaran\n",
    "\n",
    "Adapun struktur dari keseluruhan dataset yang ditampilkan yaitu :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de70986c27cbb89a45ba8a3810014d67dfd36da2"
   },
   "source": [
    "> * Data Preparation (Persiapan data)\n",
    "> * Logistic Regression\n",
    "> * Logistic Regression with Scikit Learn\n",
    "> * 2 Layer ANN\n",
    "> * 3 Layer ANN with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Preparation (Import jenis\" library yang digunakan)\n",
    "Selain import data berupa Numpy( library Python yang fokus pada scientific computing.), Pandas( sebuah open source python package/library yang menyediakan banyak perkakas untuk kebutuhan data analisis, manipulasi dan pembersihan data. ), Matlotlib(librari plotting 2D Python yang menghasilkan gambar ) seperti dibawah . Saya juga menginstall Python 3.7, tencorflow, Python Keras pada command prom anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt  # visualization\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"voice.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "0340feca6202fc9872b0ccff18a8da2f2f5b4cd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label = [1 if each == \"male\" else 0 for each in data.label]\n",
    "data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "9977ba83590556fc63b905113bc595079cce9854"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000      1  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632      1  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512      1  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119      1  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274      1  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4df72c16c16f911ba82a68f828330f29d8569b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3168 entries, 0 to 3167\n",
      "Data columns (total 21 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   meanfreq  3168 non-null   float64\n",
      " 1   sd        3168 non-null   float64\n",
      " 2   median    3168 non-null   float64\n",
      " 3   Q25       3168 non-null   float64\n",
      " 4   Q75       3168 non-null   float64\n",
      " 5   IQR       3168 non-null   float64\n",
      " 6   skew      3168 non-null   float64\n",
      " 7   kurt      3168 non-null   float64\n",
      " 8   sp.ent    3168 non-null   float64\n",
      " 9   sfm       3168 non-null   float64\n",
      " 10  mode      3168 non-null   float64\n",
      " 11  centroid  3168 non-null   float64\n",
      " 12  meanfun   3168 non-null   float64\n",
      " 13  minfun    3168 non-null   float64\n",
      " 14  maxfun    3168 non-null   float64\n",
      " 15  meandom   3168 non-null   float64\n",
      " 16  mindom    3168 non-null   float64\n",
      " 17  maxdom    3168 non-null   float64\n",
      " 18  dfrange   3168 non-null   float64\n",
      " 19  modindx   3168 non-null   float64\n",
      " 20  label     3168 non-null   int64  \n",
      "dtypes: float64(20), int64(1)\n",
      "memory usage: 519.9 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b1a06916dc7ed762ef0cb60f06e0168b74f519d1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "      <td>3168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.057126</td>\n",
       "      <td>0.185621</td>\n",
       "      <td>0.140456</td>\n",
       "      <td>0.224765</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>3.140168</td>\n",
       "      <td>36.568461</td>\n",
       "      <td>0.895127</td>\n",
       "      <td>0.408216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180907</td>\n",
       "      <td>0.142807</td>\n",
       "      <td>0.036802</td>\n",
       "      <td>0.258842</td>\n",
       "      <td>0.829211</td>\n",
       "      <td>0.052647</td>\n",
       "      <td>5.047277</td>\n",
       "      <td>4.994630</td>\n",
       "      <td>0.173752</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.016652</td>\n",
       "      <td>0.036360</td>\n",
       "      <td>0.048680</td>\n",
       "      <td>0.023639</td>\n",
       "      <td>0.042783</td>\n",
       "      <td>4.240529</td>\n",
       "      <td>134.928661</td>\n",
       "      <td>0.044980</td>\n",
       "      <td>0.177521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029918</td>\n",
       "      <td>0.032304</td>\n",
       "      <td>0.019220</td>\n",
       "      <td>0.030077</td>\n",
       "      <td>0.525205</td>\n",
       "      <td>0.063299</td>\n",
       "      <td>3.521157</td>\n",
       "      <td>3.520039</td>\n",
       "      <td>0.119454</td>\n",
       "      <td>0.500079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.018363</td>\n",
       "      <td>0.010975</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.042946</td>\n",
       "      <td>0.014558</td>\n",
       "      <td>0.141735</td>\n",
       "      <td>2.068455</td>\n",
       "      <td>0.738651</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039363</td>\n",
       "      <td>0.055565</td>\n",
       "      <td>0.009775</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.041954</td>\n",
       "      <td>0.169593</td>\n",
       "      <td>0.111087</td>\n",
       "      <td>0.208747</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>1.649569</td>\n",
       "      <td>5.669547</td>\n",
       "      <td>0.861811</td>\n",
       "      <td>0.258041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163662</td>\n",
       "      <td>0.116998</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.419828</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.070312</td>\n",
       "      <td>2.044922</td>\n",
       "      <td>0.099766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.059155</td>\n",
       "      <td>0.190032</td>\n",
       "      <td>0.140286</td>\n",
       "      <td>0.225684</td>\n",
       "      <td>0.094280</td>\n",
       "      <td>2.197101</td>\n",
       "      <td>8.318463</td>\n",
       "      <td>0.901767</td>\n",
       "      <td>0.396335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184838</td>\n",
       "      <td>0.140519</td>\n",
       "      <td>0.046110</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.765795</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>4.992188</td>\n",
       "      <td>4.945312</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.067020</td>\n",
       "      <td>0.210618</td>\n",
       "      <td>0.175939</td>\n",
       "      <td>0.243660</td>\n",
       "      <td>0.114175</td>\n",
       "      <td>2.931694</td>\n",
       "      <td>13.648905</td>\n",
       "      <td>0.928713</td>\n",
       "      <td>0.533676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199146</td>\n",
       "      <td>0.169581</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>0.277457</td>\n",
       "      <td>1.177166</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>7.007812</td>\n",
       "      <td>6.992188</td>\n",
       "      <td>0.209183</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.115273</td>\n",
       "      <td>0.261224</td>\n",
       "      <td>0.247347</td>\n",
       "      <td>0.273469</td>\n",
       "      <td>0.252225</td>\n",
       "      <td>34.725453</td>\n",
       "      <td>1309.612887</td>\n",
       "      <td>0.981997</td>\n",
       "      <td>0.842936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251124</td>\n",
       "      <td>0.237636</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>0.279114</td>\n",
       "      <td>2.957682</td>\n",
       "      <td>0.458984</td>\n",
       "      <td>21.867188</td>\n",
       "      <td>21.843750</td>\n",
       "      <td>0.932374</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          meanfreq           sd       median          Q25          Q75  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.180907     0.057126     0.185621     0.140456     0.224765   \n",
       "std       0.029918     0.016652     0.036360     0.048680     0.023639   \n",
       "min       0.039363     0.018363     0.010975     0.000229     0.042946   \n",
       "25%       0.163662     0.041954     0.169593     0.111087     0.208747   \n",
       "50%       0.184838     0.059155     0.190032     0.140286     0.225684   \n",
       "75%       0.199146     0.067020     0.210618     0.175939     0.243660   \n",
       "max       0.251124     0.115273     0.261224     0.247347     0.273469   \n",
       "\n",
       "               IQR         skew         kurt       sp.ent          sfm  ...  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000  ...   \n",
       "mean      0.084309     3.140168    36.568461     0.895127     0.408216  ...   \n",
       "std       0.042783     4.240529   134.928661     0.044980     0.177521  ...   \n",
       "min       0.014558     0.141735     2.068455     0.738651     0.036876  ...   \n",
       "25%       0.042560     1.649569     5.669547     0.861811     0.258041  ...   \n",
       "50%       0.094280     2.197101     8.318463     0.901767     0.396335  ...   \n",
       "75%       0.114175     2.931694    13.648905     0.928713     0.533676  ...   \n",
       "max       0.252225    34.725453  1309.612887     0.981997     0.842936  ...   \n",
       "\n",
       "          centroid      meanfun       minfun       maxfun      meandom  \\\n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000   \n",
       "mean      0.180907     0.142807     0.036802     0.258842     0.829211   \n",
       "std       0.029918     0.032304     0.019220     0.030077     0.525205   \n",
       "min       0.039363     0.055565     0.009775     0.103093     0.007812   \n",
       "25%       0.163662     0.116998     0.018223     0.253968     0.419828   \n",
       "50%       0.184838     0.140519     0.046110     0.271186     0.765795   \n",
       "75%       0.199146     0.169581     0.047904     0.277457     1.177166   \n",
       "max       0.251124     0.237636     0.204082     0.279114     2.957682   \n",
       "\n",
       "            mindom       maxdom      dfrange      modindx        label  \n",
       "count  3168.000000  3168.000000  3168.000000  3168.000000  3168.000000  \n",
       "mean      0.052647     5.047277     4.994630     0.173752     0.500000  \n",
       "std       0.063299     3.521157     3.520039     0.119454     0.500079  \n",
       "min       0.004883     0.007812     0.000000     0.000000     0.000000  \n",
       "25%       0.007812     2.070312     2.044922     0.099766     0.000000  \n",
       "50%       0.023438     4.992188     4.945312     0.139357     0.500000  \n",
       "75%       0.070312     7.007812     6.992188     0.209183     1.000000  \n",
       "max       0.458984    21.867188    21.843750     0.932374     1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "083e50f079aa0ab7f5dadb43b0b69b0f894889a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  (3168,)\n",
      "x:  (3168, 20)\n"
     ]
    }
   ],
   "source": [
    "y = data.label.values\n",
    "x = data.drop(['label'],axis=1)\n",
    "x = (x-np.min(x))/(np.max(x)-np.min(x)).values  # Normalize\n",
    "\n",
    "print(\"y: \", y.shape)\n",
    "print(\"x: \", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "27350f10995e3060c56a4e41f7dfbaacbfdee451"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logika regresi adalah salah satu analisis yang paling populer dan luas pemakaiannya. Regresi juga dapat berlogika garis tengah yang memisahkan data dari sebuah data seperti sumbu x dan y berupa sekumpulan titik dan garis pembagi antara titik tersebut yang dillogikankan sebagai regresi .Analisis regresi juga dipakai secara luas untuk melakukan prediksi dan ramalan, dengan penggunaan yang saling melengkapi dengan bidang pembelajaran mesin. Analisis ini juga digunakan untuk memahami variabel bebas mana saja yang berhubungan dengan variabel terikat, dan untuk mengetahui bentuk-bentuk hubungan tersebut. \n",
    "\n",
    "### Weight dan Bias\n",
    "\n",
    "Weight adalah parameter dalam jaringan neural yang mengubah data masukan dalam lapisan tersembunyi jaringan. Jaringan saraf adalah serangkaian node, atau neuron. Dalam setiap node adalah satu set input, bobot, dan nilai bias. \n",
    "\n",
    "Bias merupakan nilai konstan yang ditambahkan ke produk input dan bobot. Bias digunakan untuk mengimbangi hasil. Bias digunakan untuk menggeser hasil fungsi aktivasi ke arah positif atau negatif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "ea3342a212a55f4768d32f82061f5549a94a54c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weights:  (20, 1)\n",
      "Shape of bias:  (20, 1)\n"
     ]
    }
   ],
   "source": [
    "def init_weights_and_bias(dim):\n",
    "    '''\n",
    "    Shape of weights:  (20, 1)\n",
    "    Shape of bias:  (20, 1)\n",
    "    '''\n",
    "    weights = np.full((dim, 1),0.01)\n",
    "    bias = np.zeros(dim).reshape(-1, 1)\n",
    "    return weights, bias\n",
    "\n",
    "weights, bias = init_weights_and_bias(20)\n",
    "print(\"Shape of weights: \", weights.shape)\n",
    "print(\"Shape of bias: \", bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "de5037dd78ba4276aa77e6875631540db9dccf75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  (20, 2534)\n",
      "labels:  (2534,)\n",
      "test_features:  (20, 634)\n",
      "test_labels:  (634,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "features = x_train.T\n",
    "labels = y_train.T\n",
    "test_features = x_test.T\n",
    "test_labels = y_test.T\n",
    "\n",
    "print(\"features: \", features.shape)\n",
    "print(\"labels: \", labels.shape)\n",
    "print(\"test_features: \", test_features.shape)\n",
    "print(\"test_labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "sigmoid bersifat umum dalam statistik sebagai fungsi distribusi kumulatif (yang berubah dari 0 ke 1), \n",
    "seperti integral dari kepadatan logistik , kepadatan normal , dan fungsi kepadatan probabilitas  . \n",
    "Fungsi sigmoid logistik dapat dibalik, dan kebalikannya adalah fungsi logit. Fungsi lain sigmoid yaitu dapat memiliki domain semua bilangan real , dengan nilai balik (respon) yang umumnya meningkat secara monoton tetapi bisa juga menurun. Fungsi sigmoid paling sering menunjukkan nilai kembali (sumbu y) dalam rentang 0 hingga 1. Rentang lain yang umum digunakan adalah dari âˆ’1 hingga 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "2a4d0a6b564bcded13774e05f17e568f7ac141b6"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    yHat = 1 / (1 + np.exp(-Z))\n",
    "    return yHat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Back Propagation \n",
    "\n",
    "Backpropagation ini merupakan perkembangan dari single layer network (Jaringan Layar Tunggal) yang memiliki dua layer, yaitu input layer dan output layer. Adapun Back propagation memiliki tiga algoritma utama berupa Pengambilan input, Penelusuran error, Penyesuaian bobot. Backpropagation juga merupakan sebuah metode sistematik untuk pelatihan multilayer JST. Backpopagation dikatakan sebagai algoritma pelatihan multilayer karena Backpropagation memiliki tiga layer dalam proses pelatihannya, yaitu input layer, hidden layer dan output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "4634a0b0633d5acb61bfd87a9fb0831229bda7f0"
   },
   "outputs": [],
   "source": [
    "def feedforward_back_prop(weights, bias, features, labels):\n",
    "    '''\n",
    "    features:  (20, 2534)\n",
    "    weights: (20, 1)\n",
    "    weights.T: (1, 20)\n",
    "    bias: (20, 1)\n",
    "    yHat: (1, 2534)\n",
    "    '''\n",
    "    # Feed Forward Propagation\n",
    "    Z = np.dot(weights.T, features ) + bias\n",
    "    yHat = sigmoid(Z)\n",
    "    # Cost Function\n",
    "    loss = -labels*np.log(yHat)-(1-labels)*np.log(1-yHat)\n",
    "    cost = (np.sum(loss))/features.shape[1]\n",
    "    # Backward Propagation\n",
    "    dW = (np.dot(features, ((yHat-labels).T)))/features.shape[1]\n",
    "    dB = np.sum(yHat-labels)/features.shape[1]\n",
    "    grads = {\"dW\": dW, \"dB\": dB}\n",
    "    return cost, grads\n",
    "\n",
    "cost, grads = feedforward_back_prop(weights, bias, features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "d4507bfaa228fb80ffe898ff2d94f02dec609dd6"
   },
   "outputs": [],
   "source": [
    "def update(weights, bias, features, labels, lr, reiter):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "    \n",
    "    # Updating (learning) parameters is number_of_iterations times\n",
    "    for i in range(reiter):\n",
    "        \n",
    "        cost, grads = feedforward_back_prop(weights, bias, features, labels)\n",
    "        #cost = cost_function(features, labels, weights, bias)\n",
    "        cost_list.append(cost)\n",
    "        #Let's update\n",
    "        weights = weights - lr * grads[\"dW\"]\n",
    "        bias = bias - lr * grads[\"dB\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print(\"Cost after iterations %i: %f\" %(i, cost))\n",
    "            \n",
    "    # We update (learn) parameters weights and bias\n",
    "    parameters = {\"weights\": weights, \"bias\": bias}\n",
    "    plt.plot(index, cost_list2)\n",
    "    plt.xticks(index, rotation = \"vertical\")\n",
    "    plt.xlabel(\"Number of iterations\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, grads, cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logika regresi \n",
    "Logika regresi adalah salah satu analisis yang paling populer dan luas pemakaiannya. Regresi juga dapat berlogika garis tengah yang memisahkan data dari sebuah data seperti sumbu x dan y berupa sekumpulan titik dan garis pembagi antara titik tersebut yang dillogikankan sebagai regresi .Analisis regresi juga dipakai secara luas untuk melakukan prediksi dan ramalan, dengan penggunaan yang saling melengkapi dengan bidang pembelajaran mesin. Analisis ini juga digunakan untuk memahami variabel bebas mana saja yang berhubungan dengan variabel terikat, dan untuk mengetahui bentuk-bentuk hubungan tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "87812cf885b3360285f1fa56a38c1b383e61850a"
   },
   "outputs": [],
   "source": [
    "def logistic_regression(features, labels, test_features, test_labels, lr ,  reiter):\n",
    "    # Initialize\n",
    "    # lr: learning rate\n",
    "    \n",
    "    dim =  features.shape[0]\n",
    "    # dim =  features.shape[0]: 20 for our case\n",
    "    weights, bias = init_weights_and_bias(dim)\n",
    "    # Shape of weights:  (20, 1)\n",
    "    # Shape of bias:  (20, 1)\n",
    "    parameters, grads, cost_list = update(weights, bias, features, labels, lr, reiter)\n",
    "    \n",
    "    prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"],test_features)\n",
    "\n",
    "    # Print test Errors\n",
    "    print(\"Model A test accuracy: {} %\".format(100 - np.mean(np.abs(prediction_test - test_labels)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediksi Akurasi\n",
    "\n",
    "Pada program dibawah menampilkan prediksi akurasi untuk pengambilan keputusan strategis seperti manajemen inventaris, perencanaan anggaran, manajemen hubungan pelanggan, promosi pemasaran, dan alokasi sumber daya yang efisien yang mana data deret waktu sangat penting untuk tingkat ketinggian akurasi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "116f8d294ec115fe33c091e94fd74726ad047a7f"
   },
   "outputs": [],
   "source": [
    "def predict(weights, bias, test_features):\n",
    "    # test_features are a input for feed forward propagation\n",
    "    Z = sigmoid(np.dot(weights.T, test_features) + bias)\n",
    "    prediction = np.zeros((1, test_features.shape[1]))\n",
    "    for i in range(Z.shape[1]):\n",
    "        if Z[0, i] <= 0.5:\n",
    "            prediction[0, i] = 0\n",
    "        else:\n",
    "            prediction[0, i] = 1  \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output grafik Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "583e4840bcb43c1dd763c6b96be4f59b1101b0ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iterations 0: 13.914662\n",
      "Cost after iterations 10: 13.511259\n",
      "Cost after iterations 20: 13.184068\n",
      "Cost after iterations 30: 12.888414\n",
      "Cost after iterations 40: 12.620282\n",
      "Cost after iterations 50: 12.376160\n",
      "Cost after iterations 60: 12.152998\n",
      "Cost after iterations 70: 11.948153\n",
      "Cost after iterations 80: 11.759345\n",
      "Cost after iterations 90: 11.584609\n",
      "Cost after iterations 100: 11.422252\n",
      "Cost after iterations 110: 11.270813\n",
      "Cost after iterations 120: 11.129036\n",
      "Cost after iterations 130: 10.995831\n",
      "Cost after iterations 140: 10.870259\n",
      "Cost after iterations 150: 10.751504\n",
      "Cost after iterations 160: 10.638857\n",
      "Cost after iterations 170: 10.531702\n",
      "Cost after iterations 180: 10.429499\n",
      "Cost after iterations 190: 10.331779\n",
      "Cost after iterations 200: 10.238127\n",
      "Cost after iterations 210: 10.148180\n",
      "Cost after iterations 220: 10.061618\n",
      "Cost after iterations 230: 9.978158\n",
      "Cost after iterations 240: 9.897549\n",
      "Cost after iterations 250: 9.819568\n",
      "Cost after iterations 260: 9.744017\n",
      "Cost after iterations 270: 9.670718\n",
      "Cost after iterations 280: 9.599514\n",
      "Cost after iterations 290: 9.530262\n",
      "Cost after iterations 300: 9.462835\n",
      "Cost after iterations 310: 9.397117\n",
      "Cost after iterations 320: 9.333005\n",
      "Cost after iterations 330: 9.270404\n",
      "Cost after iterations 340: 9.209229\n",
      "Cost after iterations 350: 9.149402\n",
      "Cost after iterations 360: 9.090854\n",
      "Cost after iterations 370: 9.033518\n",
      "Cost after iterations 380: 8.977336\n",
      "Cost after iterations 390: 8.922254\n",
      "Cost after iterations 400: 8.868222\n",
      "Cost after iterations 410: 8.815194\n",
      "Cost after iterations 420: 8.763127\n",
      "Cost after iterations 430: 8.711983\n",
      "Cost after iterations 440: 8.661724\n",
      "Cost after iterations 450: 8.612318\n",
      "Cost after iterations 460: 8.563732\n",
      "Cost after iterations 470: 8.515938\n",
      "Cost after iterations 480: 8.468908\n",
      "Cost after iterations 490: 8.422616\n",
      "Cost after iterations 500: 8.377039\n",
      "Cost after iterations 510: 8.332154\n",
      "Cost after iterations 520: 8.287939\n",
      "Cost after iterations 530: 8.244376\n",
      "Cost after iterations 540: 8.201444\n",
      "Cost after iterations 550: 8.159126\n",
      "Cost after iterations 560: 8.117405\n",
      "Cost after iterations 570: 8.076265\n",
      "Cost after iterations 580: 8.035691\n",
      "Cost after iterations 590: 7.995669\n",
      "Cost after iterations 600: 7.956184\n",
      "Cost after iterations 610: 7.917223\n",
      "Cost after iterations 620: 7.878774\n",
      "Cost after iterations 630: 7.840825\n",
      "Cost after iterations 640: 7.803364\n",
      "Cost after iterations 650: 7.766380\n",
      "Cost after iterations 660: 7.729863\n",
      "Cost after iterations 670: 7.693803\n",
      "Cost after iterations 680: 7.658188\n",
      "Cost after iterations 690: 7.623011\n",
      "Cost after iterations 700: 7.588262\n",
      "Cost after iterations 710: 7.553933\n",
      "Cost after iterations 720: 7.520014\n",
      "Cost after iterations 730: 7.486498\n",
      "Cost after iterations 740: 7.453376\n",
      "Cost after iterations 750: 7.420642\n",
      "Cost after iterations 760: 7.388287\n",
      "Cost after iterations 770: 7.356305\n",
      "Cost after iterations 780: 7.324689\n",
      "Cost after iterations 790: 7.293431\n",
      "Cost after iterations 800: 7.262526\n",
      "Cost after iterations 810: 7.231966\n",
      "Cost after iterations 820: 7.201746\n",
      "Cost after iterations 830: 7.171860\n",
      "Cost after iterations 840: 7.142302\n",
      "Cost after iterations 850: 7.113066\n",
      "Cost after iterations 860: 7.084147\n",
      "Cost after iterations 870: 7.055538\n",
      "Cost after iterations 880: 7.027236\n",
      "Cost after iterations 890: 6.999234\n",
      "Cost after iterations 900: 6.971529\n",
      "Cost after iterations 910: 6.944114\n",
      "Cost after iterations 920: 6.916986\n",
      "Cost after iterations 930: 6.890139\n",
      "Cost after iterations 940: 6.863569\n",
      "Cost after iterations 950: 6.837271\n",
      "Cost after iterations 960: 6.811242\n",
      "Cost after iterations 970: 6.785477\n",
      "Cost after iterations 980: 6.759972\n",
      "Cost after iterations 990: 6.734722\n",
      "Cost after iterations 1000: 6.709725\n",
      "Cost after iterations 1010: 6.684975\n",
      "Cost after iterations 1020: 6.660470\n",
      "Cost after iterations 1030: 6.636205\n",
      "Cost after iterations 1040: 6.612177\n",
      "Cost after iterations 1050: 6.588383\n",
      "Cost after iterations 1060: 6.564818\n",
      "Cost after iterations 1070: 6.541480\n",
      "Cost after iterations 1080: 6.518365\n",
      "Cost after iterations 1090: 6.495470\n",
      "Cost after iterations 1100: 6.472791\n",
      "Cost after iterations 1110: 6.450326\n",
      "Cost after iterations 1120: 6.428072\n",
      "Cost after iterations 1130: 6.406025\n",
      "Cost after iterations 1140: 6.384183\n",
      "Cost after iterations 1150: 6.362542\n",
      "Cost after iterations 1160: 6.341100\n",
      "Cost after iterations 1170: 6.319855\n",
      "Cost after iterations 1180: 6.298802\n",
      "Cost after iterations 1190: 6.277940\n",
      "Cost after iterations 1200: 6.257266\n",
      "Cost after iterations 1210: 6.236778\n",
      "Cost after iterations 1220: 6.216473\n",
      "Cost after iterations 1230: 6.196348\n",
      "Cost after iterations 1240: 6.176400\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A test accuracy: 93.37539432176656 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(features, labels, test_features, test_labels, lr = 0.1 ,  reiter= 1250) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c07f09bc2af0d2d0c2393dbbb6c2586bd19030f1"
   },
   "source": [
    "## Logistic Regression with ScikitLearn\n",
    "Scikitlearn merupakan sebuah package yang berguna untuk pustaka pembelajaran mesin perangkat lunak gratis untuk bahasa pemrograman Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "024d37c9f1da32bff9228ea826b32404ccdec952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model B test accuracy: 0.9826498422712934\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Model_B = LogisticRegression()\n",
    "Model_B.fit(x_train,y_train)\n",
    "print(\"Model B test accuracy: {}\".format(Model_B.score(x_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94359deed20ed8ce0dd28c884fe36b0d5c5b5c5d"
   },
   "source": [
    "## 2 Layer ANN\n",
    "\n",
    "Pengoperasian model ANN menggunakan 2 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "c26f5a13c1a4801dd017ce5adc54a601a60c5052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  (20, 2534)\n",
      "labels:  (1, 2534)\n",
      "test_features:  (20, 634)\n",
      "test_labels:  (1, 634)\n"
     ]
    }
   ],
   "source": [
    "labels = y_train.reshape(y_train[0], -1)\n",
    "test_labels = y_test.reshape(y_test[1], -1)\n",
    "\n",
    "print(\"features: \", features.shape)\n",
    "print(\"labels: \", labels.shape)\n",
    "print(\"test_features: \", test_features.shape)\n",
    "print(\"test_labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metode ANN \n",
    "Jaringan syaraf tiruan adalah jaringan dari sekelompok unit pemroses kecil yang dimodelkan berdasarkan sistem saraf manusia. JST merupakan sistem adaptif yang dapat mengubah strukturnya untuk memecahkan masalah berdasarkan informasi eksternal maupun internal yang mengalir melalui jaringan tersebut.Adapun pemanfaatan model ANN seperti tampilan program dibawah.\n",
    "\n",
    "###  Forward Propagation\n",
    "Forward Propagation adalah operasi yang akan dilakukan pada setiap elemen pada input dan tiap weight yang terhubung dengan input dan ditambahkan dengan bias. Hasil dari operasi ini akan dimasukkan ke dalam activation function.\n",
    "\n",
    "### Update Parameter\n",
    "Parameter pada umumnya adalah karakteristik apa pun yang dapat membantu dalam mendefinisikan atau mengklasifikasikan sistem tertentu. Artinya, parameter adalah elemen sistem yang berguna, atau kritis, saat mengidentifikasi sistem, atau saat mengevaluasi kinerjanya, status, kondisi. Parameter juga dapat didefinisikan sebagai koefisien model, dan mereka dipilih oleh model itu sendiri. Ini berarti bahwa algoritme, saat belajar, mengoptimalkan koefisien ini (sesuai dengan strategi pengoptimalan yang diberikan) dan mengembalikan serangkaian parameter yang meminimalkan kesalahan. Contoh pemanfaatannya terdapat dalam tugas regresi linier,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "8e6e5060d1fcec4f034543e6d16f91e024909e90"
   },
   "outputs": [],
   "source": [
    "class ArtificialNeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, xTrain, xTest, yTrain, yTest):\n",
    "        # Define train and test data\n",
    "        self.xTrain = xTrain\n",
    "        self.xTest = xTest\n",
    "        self.yTrain = yTrain.reshape(yTrain.shape[0],-1)\n",
    "        self.yTest = yTest.reshape(yTest.shape[0],-1)\n",
    "\n",
    "        # Define hyperparameters\n",
    "        self.inputLayerSize = self.xTrain.shape[0] # nx <-> Number of features\n",
    "        self.hiddenLayerSize = 4\n",
    "        self.outputLayerSize = self.yTrain.shape[0]\n",
    "        \n",
    "    def initializeWeightsAndBias(self): #, inputLayerSize, hiddenLayerSize, outputLayerSize):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (inputLayerSize, 1) for w and initializes b to 0.\n",
    "\n",
    "        Argument:\n",
    "        inputLayerSize -- size of the input layer\n",
    "        hiddenLayerSize -- size of the hidden layer\n",
    "        outputLayerSize -- size of the output layer\n",
    "\n",
    "        Returns:\n",
    "        params -- python dictionary containing your parameters:\n",
    "                        W1 -- weight matrix of shape (hiddenLayerSize, inputLayerSize)\n",
    "                        b1 -- bias vector of shape (hiddenLayerSize, 1)\n",
    "                        W2 -- weight matrix of shape (outputLayerSize, hiddenLayerSize)\n",
    "                        b2 -- bias vector of shape (outputLayerSize, 1)\n",
    "        \"\"\"\n",
    "        np.random.seed(23) # We set up a seed so that your output matches ours \n",
    "                           # although the initialization is random.\n",
    "        \n",
    "        W1 = np.random.randn(self.inputLayerSize, \n",
    "                             self.hiddenLayerSize) * 0.01\n",
    "        b1 = np.zeros(shape=(self.hiddenLayerSize, 1))\n",
    "        W2 = np.random.randn(self.hiddenLayerSize,\n",
    "                             self.outputLayerSize) * 0.01\n",
    "        b2 = np.zeros(shape=(self.outputLayerSize, 1))\n",
    "        \n",
    "        # assert(isinstance(B1, float) or isinstance(B1, int))\n",
    "        \n",
    "        assert (W1.shape == (self.inputLayerSize, self.hiddenLayerSize)), \"[W1] -> Unsuitable matrix size\"\n",
    "        assert (b1.shape == (self.hiddenLayerSize, 1))\n",
    "        assert (W2.shape == (self.hiddenLayerSize, self.outputLayerSize)), \"[W2] -> Unsuitable matrix size\"\n",
    "        assert (b2.shape == (self.outputLayerSize, 1))\n",
    "        \n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2}   \n",
    "        \n",
    "        return parameters\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \"\"\" Apply and compute sigmoid activation function to scalar, vector, or matrix (Z)\n",
    "\n",
    "        Arguments:\n",
    "        Z -- A scalar or numpy array of any size.\n",
    "\n",
    "        Return:\n",
    "        s -- sigmoid(z)\n",
    "        \"\"\"\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    def forwardPropagation(self, X, parameters):\n",
    "        \"\"\" Propogate inputs though network\n",
    "        \n",
    "        Argument:\n",
    "        X -- input data of size (inputLayerSize, m)\n",
    "        parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "\n",
    "        Returns:\n",
    "        A2 -- The sigmoid output of the second activation\n",
    "        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "        \"\"\"\n",
    "        # Retrieve each parameter from the dictionary \"parameters\"\n",
    "        W1 = parameters['W1']\n",
    "        b1 = parameters['b1']\n",
    "        W2 = parameters['W2']\n",
    "        b2 = parameters['b2']\n",
    "\n",
    "        # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "        Z1 = np.dot(W1.T, X) + b1\n",
    "        A1 = sigmoid(Z1)\n",
    "        Z2 = np.dot(W2.T, A1) + b2\n",
    "        yHat = self.sigmoid(Z2) # A2\n",
    "\n",
    "        assert(yHat.shape == (1, X.shape[1]))\n",
    "    \n",
    "        cache = {\"Z1\": Z1,\n",
    "                 \"A1\": A1,\n",
    "                 \"Z2\": Z2,\n",
    "                 \"yHat\": yHat}    # A2\n",
    "    \n",
    "        return yHat, cache\n",
    "    \n",
    "    def computeCost(self, yHat, Y, parameters):\n",
    "        \"\"\" Compute cost for given X,Y, use weights already stored in class \n",
    "\n",
    "        Arguments:\n",
    "        yHat -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "        Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "        parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "\n",
    "        Returns:\n",
    "        cost -- cross-entropy cost given equation (13)\n",
    "        \"\"\"\n",
    "        m = Y.shape[1] # number of example\n",
    "                      \n",
    "        # Retrieve W1 and W2 from parameters\n",
    "        W1 = parameters['W1']\n",
    "        W2 = parameters['W2']   \n",
    "                    \n",
    "        # Loss\n",
    "        logprobs = np.multiply(np.log(yHat), Y) + np.multiply((1 - Y), np.log(1 - yHat))\n",
    "        # Cost\n",
    "        cost = - (np.sum(logprobs)) / m     # m =  yTrain.shape[1]  is for scaling\n",
    "        \n",
    "        cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "        assert(isinstance(cost, float))\n",
    "                      \n",
    "        return cost\n",
    "\n",
    "    def backwardPropagation(self,parameters, cache,  X, Y):\n",
    "        \"\"\" Compute the gradients of parameters by implementing the backward propagation\n",
    "\n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing our parameters \n",
    "        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "        X -- input data of shape (2, number of examples)\n",
    "        Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        grads -- python dictionary containing your gradients with respect to different parameters\n",
    "        \"\"\"\n",
    "        m = X.shape[1]   \n",
    "                      \n",
    "        # First, retrieve W1 and W2 from the dictionary \"parameters\".       \n",
    "        W1 = parameters['W1']\n",
    "        W2 = parameters['W2']\n",
    "                      \n",
    "        # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "        A1 = cache['A1']\n",
    "        yHat = cache['yHat']                    \n",
    "                      \n",
    "        # Backward propagation: calculate dW1, db1, dW2, db2.                     \n",
    "        dZ2 = yHat - Y\n",
    "        dW2 = (1 / m) * np.dot(A1, dZ2.T)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.multiply(np.dot(W2, dZ2), 1 - np.power(A1, 2))\n",
    "        dW1 = (1 / m) * np.dot(X, dZ1.T)#(1 / m) * np.dot(dZ1, self.xTrain.T) # MATRIS BOYUTLARINA BAK dW1 ve dW2 ICIN\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)   # m is for scaling \n",
    "\n",
    "        gradients = {\"dW1\": dW1,\n",
    "                     \"db1\": db1,\n",
    "                     \"dW2\": dW2,\n",
    "                     \"db2\": db2}\n",
    "                      \n",
    "        return gradients\n",
    "    \n",
    "    def updateParameters(self, parameters, gradients, learning_rate = 0.15):\n",
    "        \"\"\"\n",
    "        Updates parameters using the gradient descent update rule given above\n",
    "\n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing your parameters \n",
    "        grads -- python dictionary containing your gradients \n",
    "\n",
    "        Returns:\n",
    "        parameters -- python dictionary containing your updated parameters \n",
    "        \"\"\"\n",
    "        # Retrieve each parameter from the dictionary \"parameters\"\n",
    "        W1 = parameters['W1']\n",
    "        b1 = parameters['b1']\n",
    "        W2 = parameters['W2']\n",
    "        b2 = parameters['b2']\n",
    "\n",
    "        # Retrieve each gradient from the dictionary \"grads\"\n",
    "        dW1 = gradients['dW1']\n",
    "        db1 = gradients['db1']\n",
    "        dW2 = gradients['dW2']\n",
    "        db2 = gradients['db2']\n",
    "        \n",
    "        # Update rule for each parameter\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "        W2 = W2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "\n",
    "        parameters = {\"W1\": W1,\n",
    "                      \"b1\": b1,\n",
    "                      \"W2\": W2,\n",
    "                      \"b2\": b2}\n",
    "\n",
    "        return parameters\n",
    "                      \n",
    "    def model(self, X, Y, num_iterations=10000, print_cost=False):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        X -- dataset of shape (2, number of examples)\n",
    "        Y -- labels of shape (1, number of examples)\n",
    "        n_h -- size of the hidden layer\n",
    "        num_iterations -- Number of iterations in gradient descent loop\n",
    "        print_cost -- if True, print the cost every 1000 iterations\n",
    "\n",
    "        Returns:\n",
    "        parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "        \"\"\"\n",
    "        np.random.seed(3)\n",
    "        \n",
    "        costStr = []\n",
    "        indexStr = []\n",
    "        \n",
    "        # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "        parameters = self.initializeWeightsAndBias()\n",
    "\n",
    "        W1 = parameters['W1']\n",
    "        b1 = parameters['b1']\n",
    "        W2 = parameters['W2']\n",
    "        b2 = parameters['b2']\n",
    " \n",
    "        # Loop (gradient descent)\n",
    "        for i in range(0, num_iterations):\n",
    "                      \n",
    "            # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "            yHat, cache = self.forwardPropagation(X, parameters)\n",
    "\n",
    "            # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "            cost = self.computeCost(yHat, Y, parameters)\n",
    "            \n",
    "            # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "            gradients = self.backwardPropagation(parameters, cache, X, Y)\n",
    "\n",
    "            # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "            parameters = self.updateParameters(parameters, gradients)\n",
    "\n",
    "            # Print the cost every 1000 iterations\n",
    "            if print_cost and i % 1000 == 0:\n",
    "                costStr.append(cost)\n",
    "                indexStr.append(i)\n",
    "                print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            \"\"\"\n",
    "            # Plot Cost Function\n",
    "            plt.plot(indexStr,costStr)\n",
    "            plt.xticks(indexStr,rotation='vertical')\n",
    "            plt.xlabel(\"Number of Iterarion\")\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.show()\n",
    "            \"\"\"\n",
    "        return parameters\n",
    "\n",
    "    def predict(self, parameters, X):\n",
    "        \"\"\"\n",
    "        Using the learned parameters, predicts a class for each example in X\n",
    "\n",
    "        Arguments:\n",
    "        parameters -- python dictionary containing your parameters \n",
    "        X -- input data of size (n_x, m)\n",
    "\n",
    "        Returns\n",
    "        predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "        \"\"\"\n",
    "        # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "        yHat, cache = self.forwardPropagation(X, parameters)\n",
    "        predictions = np.round(yHat)\n",
    "\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693338\n",
      "Cost after iteration 1000: 0.537436\n",
      "Cost after iteration 2000: 0.454870\n",
      "Cost after iteration 3000: 0.347725\n",
      "Cost after iteration 4000: 0.290624\n",
      "Cost after iteration 5000: 0.256894\n",
      "Cost after iteration 6000: 0.226229\n",
      "Cost after iteration 7000: 0.164414\n",
      "Cost after iteration 8000: 0.118214\n",
      "Cost after iteration 9000: 0.126296\n",
      "Cost after iteration 10000: 0.127759\n",
      "Cost after iteration 11000: 0.121675\n",
      "Train Accuracy: 97%\n"
     ]
    }
   ],
   "source": [
    "ANN = ArtificialNeuralNetwork(features, test_features, labels, test_labels)\n",
    "parameters = ANN.model(features, labels, num_iterations = 12000, print_cost=True)\n",
    "predictions = ANN.predict(parameters, features)\n",
    "print('Train Accuracy: %d' % float((np.dot(labels, predictions.T) + np.dot(1 - labels, 1 - predictions.T)) / float(labels.size) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.692609\n",
      "Cost after iteration 1000: 0.593128\n",
      "Cost after iteration 2000: 0.378382\n",
      "Cost after iteration 3000: 0.301035\n",
      "Cost after iteration 4000: 0.277850\n",
      "Cost after iteration 5000: 0.265836\n",
      "Cost after iteration 6000: 0.256918\n",
      "Cost after iteration 7000: 0.247743\n",
      "Cost after iteration 8000: 0.201890\n",
      "Cost after iteration 9000: 0.229451\n",
      "Cost after iteration 10000: 0.223250\n",
      "Cost after iteration 11000: 0.258326\n",
      "Test Accuracy: 89%\n"
     ]
    }
   ],
   "source": [
    "parameters = ANN.model(test_features, test_labels, num_iterations = 12000, print_cost=True)\n",
    "predictions = ANN.predict(parameters, test_features)\n",
    "print(\"Test Accuracy: %d\" % float((np.dot(test_labels, predictions.T) + np.dot(1 - test_labels, 1 - predictions.T)) / float(test_labels.size) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hasil Kesimpulan \n",
    "Dari dataset pemanfaatan Logika rekresi dan ANN untuk Penjenisan gender, menghasilkan output akhir berupa pengklasifikasian jenis kelamin berdasarkan ciri-cirinya berjalan dengan mulus, dengan tingkat akurasi kebenaran yang tinggi berkisar 89% - 97%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
