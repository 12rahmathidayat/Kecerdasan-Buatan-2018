{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe30c858-0fff-40ee-899f-6306887293da",
    "_uuid": "8aa94cc48c58151c779a7e4967d3f6e204d158e0"
   },
   "source": [
    "# Spam SMS Classification using KNN\n",
    "**Nama : Muhammad Aldi Pangestu**\n",
    "\n",
    "**Nim : 09011281823068**\n",
    "\n",
    "**Sumber: https://www.kaggle.com/uciml/sms-spam-collection-dataset**\n",
    "\n",
    "**Sumber CSV : https://storage.googleapis.com/kagglesdsdata/datasets/483/982/spam.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20201208%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20201208T174421Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=58521da4caae1ea35e61fe97197bd179a9d09f0e208009a2808c0fd2dfcb2d485a121474a882764e414d6ed17ce9d02bf5a18234889329e37af09d31a6b9447487fdac22980db18756a8f8dbb3c96678e5ab22d269f787b9b92bd079dcf9c4ba818f0e1a98a7175fe8be2800063562852bee4cb3b8e3463f2465fcd2a95f720a80b79e300f4b3e71a6c9ade0caf4293fda70b7800fb2023861a0dd63eeb73ea958efca83fbab66a86ae6b768bf5d8136ecb188d9b86f35125dd0b2c9c2f016e82ad50223e9f7d7f49295db154022d8e2dbe4c0c2f213a78236e57002a3908b20c422ae82ca93f8e22369aae8ceb1d6c1343c53352bc3d98929d45330e15b0720**\n",
    "\n",
    "**Sumber CSV : https://www.kaggle.com/madpiano/spam-sms-classification-using-knn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d6b1ab40-34da-4645-98c4-4b61c97895f6",
    "_uuid": "0259bd59e4fad41ffd78aa5a510a83ef07daab7f",
    "collapsed": true
   },
   "source": [
    "Start from loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "a89760f4-9075-42be-b4c4-c8c87d175c88",
    "_uuid": "4429a082ddabcb23261daec29ee1ae9e68ba2a2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show what kind of data we are dealing with\n",
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "Now print each SMS text after train/test split\n",
      "1114    No no:)this is kallis home ground.amla home to...\n",
      "3589    I am in escape theatre now. . Going to watch K...\n",
      "3095    We walked from my moms. Right on stagwood pass...\n",
      "1012       I dunno they close oredi not... ÌÏ v ma fan...\n",
      "3320                               Yo im right by yo work\n",
      "4130    \\Its Ur luck to Love someone. Its Ur fortune t...\n",
      "1197     He also knows about lunch menu only da. . I know\n",
      "5426        Oh yeah! And my diet just flew out the window\n",
      "624     Nah it's straight, if you can just bring bud o...\n",
      "2260    SplashMobile: Choose from 1000s of gr8 tones e...\n",
      "Name: v2, dtype: object\n",
      "\n",
      "Now print labels of SMS texts\n",
      "1114     ham\n",
      "3589     ham\n",
      "3095     ham\n",
      "1012     ham\n",
      "3320     ham\n",
      "4130     ham\n",
      "1197     ham\n",
      "5426     ham\n",
      "624      ham\n",
      "2260    spam\n",
      "Name: v1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "print ('show what kind of data we are dealing with')\n",
    "print (df.head())\n",
    "\n",
    "# split into train and test\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(\n",
    "    df.v2,\n",
    "    df.v1, \n",
    "    test_size=0.2, \n",
    "    random_state=0)   \n",
    "print('')\n",
    "print ('Now print each SMS text after train/test split')\n",
    "print (data_train[:10])\n",
    "print('')\n",
    "print ('Now print labels of SMS texts')\n",
    "print (labels_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "00614a72-9f5d-40ad-9e45-1a4b2f74298e",
    "_uuid": "96eeae4a1d59c0ebbdb537097102db4614beda1b"
   },
   "source": [
    "Sekarang kita ingin menghitung jumlah semua kata unik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "721995ec-f0ee-4d0f-95f1-7c6c9903ff30",
    "_uuid": "9c2746c72a6aab56a5b1efbf5f641559dbc89e14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all the unique words : 13504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def GetVocabulary(data): \n",
    "    vocab_set = set([])\n",
    "    for document in data:\n",
    "        words = document.split()\n",
    "        for word in words:\n",
    "            vocab_set.add(word) \n",
    "    return list(vocab_set)\n",
    "\n",
    "vocab_list = GetVocabulary(data_train)\n",
    "print ('Number of all the unique words : ' + str(len(vocab_list)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d7a2259d-801e-4e0d-a880-177e5b433e00",
    "_uuid": "bfd54ce276dc4227055e55e74eec3032f792165d"
   },
   "source": [
    "Sekarang, kita akan memvektorisasi setiap SMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "772db8d0-2106-43ff-a3c4-b404d25c3ab2",
    "_uuid": "5d3734d61dd084d0c8a2f46fa1c1c3c26f8ce04a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3589    I am in escape theatre now. . Going to watch K...\n",
      "Name: v2, dtype: object\n",
      "We walked from my moms. Right on stagwood pass right on winterstone left on victors hill. Address is &lt;#&gt;\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Document2Vector(vocab_list, data):\n",
    "    word_vector = np.zeros(len(vocab_list))\n",
    "    words = data.split()\n",
    "    for word in words:\n",
    "        if word in vocab_list:\n",
    "            word_vector[vocab_list.index(word)] += 1\n",
    "    return word_vector\n",
    "\n",
    "print (data_train[1:2,])\n",
    "ans = Document2Vector(vocab_list,\"the the the\")\n",
    "print (data_train.values[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5033458e-55fc-4a18-aba3-1b1ecbfce2ab",
    "_uuid": "f8e259bb2ef260294478c4ec7929cdd10b68d84a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457\n"
     ]
    }
   ],
   "source": [
    "train_matrix = []\n",
    "for document in data_train.values:\n",
    "    word_vector = Document2Vector(vocab_list, document)\n",
    "    train_matrix.append(word_vector)\n",
    "\n",
    "print (len(train_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5367c702-7954-4b16-a864-222908ddfab2",
    "_uuid": "ed22ab8ea5890c59212d5063568d7a0813c2c287"
   },
   "source": [
    "Sekarang, kita akan menggunakan model Naive Bayes untuk \"mengatur\" training set, dan mendapatkan probabilitas dari setiap fitur (kata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "45c35f19-8fb5-42ca-b5a5-9e3a10cf09cd",
    "_uuid": "773bfa1e4819a5986eda2ad09dc47f7e4decc1e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on the doc id:0\n",
      "Train on the doc id:500\n"
     ]
    }
   ],
   "source": [
    "# this is not using SKlearn model\n",
    "def NaiveBayes_train(train_matrix,labels_train):\n",
    "    num_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "    \n",
    "    spam_vector_count = np.ones(num_words);\n",
    "    ham_vector_count = np.ones(num_words)  #initialize the count as 1 for each word\n",
    "    spam_total_count = num_words;\n",
    "    ham_total_count = num_words                  #this is Laplacian smooth\n",
    "    \n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    for i in range(num_docs):\n",
    "        if i % 500 == 0:\n",
    "            print ('Train on the doc id:' + str(i))\n",
    "            \n",
    "        if labels_train[i] == 'spam':\n",
    "            ham_vector_count += train_matrix[i]\n",
    "            ham_total_count += sum(train_matrix[i])\n",
    "            ham_count += 1\n",
    "        else:\n",
    "            spam_vector_count += train_matrix[i]\n",
    "            spam_total_count += sum(train_matrix[i])\n",
    "            spam_count += 1\n",
    "    \n",
    "    print (ham_count)\n",
    "    print (spam_count)\n",
    "    \n",
    "    p_spam_vector = np.log(ham_vector_count/ham_total_count)#return probability vector\n",
    "    p_ham_vector = np.log(spam_vector_count/spam_total_count)#return a priori probabiligy\n",
    "    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs)\n",
    "\n",
    "    \n",
    "p_spam_vector, p_spam, p_ham_vector, p_ham = NaiveBayes_train(train_matrix, labels_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30ef732e-8668-414c-90d2-71b24403186b",
    "_uuid": "aef1a42f58b8aa2805bb13939416e605530c05a0"
   },
   "outputs": [],
   "source": [
    "data_test.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e4415b37-0855-49bd-8926-afe63c668268",
    "_uuid": "1d737630d3f2df2b3d28355f57cabbf594c2bf23"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def Predict(test_word_vector,p_spam_vector, p_spam, p_ham_vector, p_ham):\n",
    "    \n",
    "    spam = sum(test_word_vector * p_spam_vector) + p_spam\n",
    "    ham = sum(test_word_vector * p_ham_vector) + p_ham\n",
    "    if spam > ham:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "predictions = []\n",
    "i = 0\n",
    "for document in data_test.values:\n",
    "    if i % 200 == 0:\n",
    "        print ('Test on the doc id:' + str(i))\n",
    "    i += 1    \n",
    "    test_word_vector = Document2Vector(vocab_list, document)\n",
    "    ans = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham)\n",
    "    predictions.append(ans)\n",
    "\n",
    "print (len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "33beb08e-dfd5-4b74-81bb-4740bd9813b6",
    "_uuid": "b04b025a8ebfbfccbf85acce2714a2563b09e670"
   },
   "outputs": [],
   "source": [
    "# now, evaluate the model\n",
    "\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print (accuracy_score(labels_test, predictions))\n",
    "print (classification_report(labels_test, predictions))\n",
    "print (confusion_matrix(labels_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "667139f42e4c65b0f0f680199bcccef9c29565b7"
   },
   "source": [
    "What we learned in this study:\n",
    "    1. Naive Bayes adalah model sederhana yang memprediksi spam / ham dengan menghitung probabilitas kata kunci yang representatif\n",
    "      2. Keunggulan Naive Bayes relatif sederhana, performa bagus, pelatihan cepat\n",
    "      3. Namun, ini mengasumsikan, secara default, fitur tidak bergantung, yaitu kata dan kata tidak terkait\n",
    "      4. Salah satu metode untuk meningkatkan korelasi kata-kata adalah dengan mengubah Ngram menjadi> 1. Misalnya, \"gedung putih\" harus dianggap sebagai satu kata\n",
    "      5. Singkatnya, kelemahan Naive Bayes adalah tidak dapat mempelajari interdependensi antar fitur, karena menganggap fitur tidak bergantung satu sama lain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
