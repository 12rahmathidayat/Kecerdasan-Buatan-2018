{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAMA : Lord Budhi Dharma Sakti\n",
    "NIM  : 09011181823124\n",
    "KELAS: SK 5A INDRALAYA\n",
    "KECERDASAN BUATAN\n",
    "ARTIFICIAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Fungsi dari \"from random import seed dan from random import random\" adalah Generator angka pseudo-acak bekerja dengan melakukan beberapa operasi pada suatu nilai. Secara umum nilai ini adalah angka sebelumnya yang dihasilkan oleh generator. Namun, saat pertama kali menggunakan generator, tidak ada nilai sebelumnya.\n",
    "\n",
    "2.import numpy as np bertujuan untuk fokus pada scientific computing. NumPy memiliki kemampuan untuk membentuk objek N-dimensional array, yang mirip dengan list pada Python. Keunggulan NumPy array dibandingkan dengan list pada Python adalah konsumsi memory yang lebih kecil serta runtime yang lebih cepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to initialize simple ANN with layers organized as arrays of dictionaries\n",
    "# We start with one hidden layers for the beginning\n",
    "\n",
    "def ANN(n_inputs, n_hidden, n_outputs):\n",
    "    \n",
    "    network = list()\n",
    "    \n",
    "    hidden_layer = [{'weights': [random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    \n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Input Layer merupakan lapisan yang membawa data masuk kedalam system untuk kemudian di proses pada layer selanjutnya.\n",
    "2.Hidden layer merupakan lapisan antara input layer dan output layer, dimana artificial neuron yang memiliki sekumpulan input pembobot ‘weight’ dan prosedur untuk menghasilkan output neuron melalui activation function.\n",
    "3.Output layer merupakan lapisan terakhir dari neuron yang menghasilkan output system.\n",
    "4.Weight adalah setiap input dikalikan terlebih dahulu dengan variable yang disebut sebagai ‘weight’ (w0, w1, w2), setelah itu ketiga nya dijumlahkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217]}, {'weights': [0.49543508709194095, 0.4494910647887381, 0.651592972722763, 0.7887233511355132]}], [{'weights': [0.0938595867742349, 0.02834747652200631, 0.8357651039198697]}, {'weights': [0.43276706790505337, 0.762280082457942, 0.0021060533511106927]}, {'weights': [0.4453871940548014, 0.7215400323407826, 0.22876222127045265]}]]\n"
     ]
    }
   ],
   "source": [
    "seed(1)\n",
    "network = ANN(3, 2, 3)\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217]\n",
      "[0.49543508709194095, 0.4494910647887381, 0.651592972722763, 0.7887233511355132]\n",
      "[0.0938595867742349, 0.02834747652200631, 0.8357651039198697]\n",
      "[0.43276706790505337, 0.762280082457942, 0.0021060533511106927]\n",
      "[0.4453871940548014, 0.7215400323407826, 0.22876222127045265]\n"
     ]
    }
   ],
   "source": [
    "for layer in network:\n",
    "    \n",
    "    for neuron in layer:\n",
    "        \n",
    "        print(neuron['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Forward propagate\n",
    "# 'To calculate an output from a neural network by propagating an input signal through each layer until the output layer outputs its values. We call this forward-propagation. Forward prop can be separated into three parts: 1) Neuron activation, 2) Neuron Transfer, 3) Forward Propagation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neuron Activation\n",
    "# 'The first step is to calculate the activation of one neuron given an input.'\n",
    "\n",
    "def activate(w, inputs):\n",
    "    \n",
    "    activation = w[-1] # bias\n",
    "    \n",
    "    for i in range(len(w)-1):\n",
    "        \n",
    "        activation += w[i]*inputs[i] # computing sum over all products of weights multiplied by input  \n",
    "    \n",
    "        return activation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-linearity  we are going to use is ReLU\n",
    "\n",
    "def ReLu(activation):\n",
    "    \n",
    "    return activation *(activation > 0) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.ReLU atau Rectified Linear Unit menjadi salah satu activation function yang popular belakangan ini, Vincent Vanhoucke dalam course deep learningnya di udacity mengatakan bahwa ReLU merupakan activation function favorit.\n",
    "   f(x) = max( 0, x)\n",
    "\n",
    "Karna ReLU pada intinya hanya membuat pembatas pada bilangan nol, artinya apabila x ≤ 0 maka x = 0 dan apabila x > 0 maka x = x\n",
    "\n",
    "Ada beberapa pro dan kontra ketika kita menggunakan ReLU:\n",
    "\n",
    "   (+) ReLU sangat mempercepat proses konvergensi yang dilakukan dengan stochastic gradient descent jika dibandingkan dengan sigmoid / tanh.\n",
    "   (+) Jika kita bandingan dengan sigmoid/tanh yang memiliki operasi-operasi yang “expensive” (exponentials, etc.), ReLU bisa kita implementasikan hanya dengan membuat pembatas(threshold) pada bilangan nol.\n",
    "   (-) Sayangnya, unit ReLU bisa menjadi rapuh pada saat proses training dan bisa membuat unit tersebut “mati”. Sebagai contohnya, kita mungkin bisa menemukan bahwa 40% dari network kita “mati” (neuron yang tidak akan pernah aktif selama proses training) apabila learning rate yang kita inisialisasi terlalu tinggi. Namun apabila kita menginisialisasi learning rate kita secara tepat maka hal seperti ini jarang menjadi masalah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "# 'We need to calculate an output from an NN by propagating an input signal through each layer until the output layer'\n",
    "\n",
    "def forward_pass(network,inputs):\n",
    "    \n",
    "    for layer in network:\n",
    "        \n",
    "        input_output = []\n",
    "        \n",
    "        for neuron in layer:\n",
    "            \n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            \n",
    "            neuron['output'] = ReLu(activation)\n",
    "            \n",
    "            input_output.append(neuron['output'])\n",
    "            \n",
    "        inputs =  input_output\n",
    "        \n",
    "    return inputs \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217]}, {'weights': [0.49543508709194095, 0.4494910647887381, 0.651592972722763, 0.7887233511355132]}], [{'weights': [0.0938595867742349, 0.02834747652200631, 0.8357651039198697]}, {'weights': [0.43276706790505337, 0.762280082457942, 0.0021060533511106927]}, {'weights': [0.4453871940548014, 0.7215400323407826, 0.22876222127045265]}]]\n"
     ]
    }
   ],
   "source": [
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8975398945635457, 0.2869367876011669, 0.5218990399343759]\n"
     ]
    }
   ],
   "source": [
    "inputs = [3,0,None]\n",
    "output = forward_pass(network,inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.0938595867742349, 0.02834747652200631, 0.8357651039198697], 'output': 0.8975398945635457}, {'weights': [0.43276706790505337, 0.762280082457942, 0.0021060533511106927], 'output': 0.2869367876011669}, {'weights': [0.4453871940548014, 0.7215400323407826, 0.22876222127045265], 'output': 0.5218990399343759}]\n"
     ]
    }
   ],
   "source": [
    "print(network[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6581617580766254\n",
      "2.275028612411336\n",
      "0.8975398945635457\n",
      "0.2869367876011669\n",
      "0.5218990399343759\n"
     ]
    }
   ],
   "source": [
    "for layer in network:\n",
    "    \n",
    "    for neuron in layer:\n",
    "        \n",
    "        print(neuron['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BackProp\n",
    "# 'Our goal is to backpropagate error (to update weights) between the expected and real(the one that is obtained during the forward pass) outputs.We start with computing derivative of the activation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLuDerivative(x):\n",
    "    \n",
    "    return np.greater(x, 0).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error backprop\n",
    "# 'Since we going back, we start error calculation from the last layer'\n",
    "\n",
    "def back_prop_error(network,expected):\n",
    "    \n",
    "    for i in reversed(range(len(network))):\n",
    "        \n",
    "        layer = network[i]\n",
    "        \n",
    "        errors = list()\n",
    "        \n",
    "        # Go here in the the second iteration\n",
    "        \n",
    "        if i !=len(network)-1:\n",
    "            \n",
    "            for j in range(len(layer)):\n",
    "                \n",
    "                error = 0.0\n",
    "                \n",
    "                for neuron in network[i+1]:\n",
    "                    \n",
    "                    error += (neuron['weights'][j]*neuron['delta'])\n",
    "                    \n",
    "                errors.append(error)\n",
    "          \n",
    "        # During the first iteration we compute error on the last (output) layer of ANN\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for j in range(len(layer)):\n",
    "                \n",
    "                neuron = layer[j]\n",
    "                \n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        \n",
    "        # Move an error 'back' further to the left (closer to the input)        \n",
    "        \n",
    "        for j in range(len(layer)):\n",
    "            \n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j]*ReLuDerivative(neuron['output'])\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritma backpropagation adalah sebuah algoritma untuk memperkecil tingkat error dengan menyesuaikan bobot berdasarkan perbedaan output dan target yang diinginkan.\n",
    "\n",
    "Secara umum algoritmanya terdiri dari tiga langkah utama, yaitu :\n",
    "\n",
    "   Pengambilan input\n",
    "   Penelusuran error\n",
    "   Penyesuaian bobot\n",
    "   \n",
    "Pada pengambilan input, terlebih dahulu dilakukan inisialisasi bobot, kemudian masuk ke dalam algoritma proses backpropagation yang terdiri dari komputasi maju yang bertujuan untuk menelusuri besarnya error dan komputasi balik untuk mengupdate dan menyesuaikan bobot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation merupakan sebuah algoritma yang berfungsi untuk melakukan perhitungan balik dari neuron keluaran agar memiliki nilai bobot yang sesuai dalam jaringan neural network. Dengan komputasi balik ini nilai error atau kesalahan bisa dikurangi dengan cukup baik. Berikut ini adalah gambaran kasar dari algoritma komputasi balik yang digunakan pada Artificial Neural Network.\n",
    "\n",
    "   Sinyal keluaran yang dihasilkan pada komputasi maju kemudian dicocokkan dan dilakukan perhitungan untuk menghitung selisih antara target dengan sinyal keluaran yang ada pada neuron keluaran.\n",
    "   Hasil perhitungan ini kemudian digunakan untuk menyesuaikan bobot hubungan antara lapisan keluaran dengan semua neuron yang berada pada lapisan tersembunyi.\n",
    "   Setelah itu kirimkan sinyal kesalahan ke dalam lapisan tersembunyi sehingga setiap neuron yang berada pada lapisan tersembunyi bisa menyesuaikan beban yang ada agar niali keluarannya mempunyai nilai yang mendekati dengan target.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algoritma proses backpropagationnya adalah sebagai berikut :\n",
    "   Unit Output (Yk, k=1,2,…..,m)\n",
    "   Menerima pola target yang bersesuaian dengan pola input\n",
    "   Menghitung informasi error :\n",
    "\n",
    "   dk = (tk – yk) f'(y_ink)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weights': [0.13436424411240122,\n",
       "    0.8474337369372327,\n",
       "    0.763774618976614,\n",
       "    0.2550690257394217],\n",
       "   'output': 0.6581617580766254},\n",
       "  {'weights': [0.49543508709194095,\n",
       "    0.4494910647887381,\n",
       "    0.651592972722763,\n",
       "    0.7887233511355132],\n",
       "   'output': 2.275028612411336}],\n",
       " [{'weights': [0.0938595867742349, 0.02834747652200631, 0.8357651039198697],\n",
       "   'output': 0.8975398945635457},\n",
       "  {'weights': [0.43276706790505337, 0.762280082457942, 0.0021060533511106927],\n",
       "   'output': 0.2869367876011669},\n",
       "  {'weights': [0.4453871940548014, 0.7215400323407826, 0.22876222127045265],\n",
       "   'output': 0.5218990399343759}]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Now the next step is to train ANN (we need to update weights by applying gradient descent algorithm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start with updating weights acc to GD: \n",
    "# weight = weight + learning_rate * error * input\n",
    "\n",
    "def weights_update(network, inputs , learning_rate):\n",
    "    \n",
    "    for i in range(len(network)):\n",
    "        \n",
    "        inputs = inputs[:-1] # Return all exept last one (bias)\n",
    "        \n",
    "        if i!=0:\n",
    "            \n",
    "            inputs = [neuron['output'] for neuron in network[i-1]]\n",
    "            \n",
    "        for neuron in network[i]:\n",
    "            \n",
    "            for j in range(len(inputs)):\n",
    "                \n",
    "                neuron['weights'][j] += learning_rate * neuron['delta'] * inputs[j]\n",
    "                \n",
    "            neuron['weights'][-1] += learning_rate * neuron['delta']\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train Network\n",
    "\n",
    "# 'We need to make one forward pass, then to compute our error between the real value and what we actually expected (supervise learning approach).  This difference is reflected in a cost function with we are going to optimize by using Gradient Descent.'\n",
    "\n",
    "def train_network(network,dataset,learning_rate, epochs, n_outputs):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        cost = 0\n",
    "        \n",
    "        for row in dataset:\n",
    "            \n",
    "            outputs = forward_pass(network,row)\n",
    "            expected = [0 for i in range(n_outputs)] # Define expected class labels\n",
    "            expected[row[-1]] = 1\n",
    "            \n",
    "            cost += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))]) #MSE\n",
    "            \n",
    "            back_prop_error(network,expected)\n",
    "            \n",
    "            weights_update(network,row,learning_rate)\n",
    "            \n",
    "        print('>epoch=%d, lrate=%.3f, cost=%.3f' % (epoch, learning_rate, cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset])) # Binary problem, therefore only two outputs\n",
    "learning_rate = 1e-4\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]},\n",
       "  {'weights': [0.2550690257394217, 0.49543508709194095, 0.4494910647887381]}],\n",
       " [{'weights': [0.651592972722763, 0.7887233511355132, 0.0938595867742349]},\n",
       "  {'weights': [0.02834747652200631, 0.8357651039198697, 0.43276706790505337]}]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = ANN(inputs,2,n_outputs)\n",
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.000, cost=10.131\n",
      ">epoch=1, lrate=0.000, cost=9.957\n",
      ">epoch=2, lrate=0.000, cost=9.790\n",
      ">epoch=3, lrate=0.000, cost=9.628\n",
      ">epoch=4, lrate=0.000, cost=9.472\n",
      ">epoch=5, lrate=0.000, cost=9.322\n",
      ">epoch=6, lrate=0.000, cost=9.177\n",
      ">epoch=7, lrate=0.000, cost=9.037\n",
      ">epoch=8, lrate=0.000, cost=8.902\n",
      ">epoch=9, lrate=0.000, cost=8.771\n",
      ">epoch=10, lrate=0.000, cost=8.645\n",
      ">epoch=11, lrate=0.000, cost=8.523\n",
      ">epoch=12, lrate=0.000, cost=8.406\n",
      ">epoch=13, lrate=0.000, cost=8.292\n",
      ">epoch=14, lrate=0.000, cost=8.182\n",
      ">epoch=15, lrate=0.000, cost=8.076\n",
      ">epoch=16, lrate=0.000, cost=7.973\n",
      ">epoch=17, lrate=0.000, cost=7.873\n",
      ">epoch=18, lrate=0.000, cost=7.777\n",
      ">epoch=19, lrate=0.000, cost=7.684\n",
      ">epoch=20, lrate=0.000, cost=7.594\n",
      ">epoch=21, lrate=0.000, cost=7.506\n",
      ">epoch=22, lrate=0.000, cost=7.422\n",
      ">epoch=23, lrate=0.000, cost=7.340\n",
      ">epoch=24, lrate=0.000, cost=7.261\n",
      ">epoch=25, lrate=0.000, cost=7.184\n",
      ">epoch=26, lrate=0.000, cost=7.109\n",
      ">epoch=27, lrate=0.000, cost=7.037\n",
      ">epoch=28, lrate=0.000, cost=6.967\n",
      ">epoch=29, lrate=0.000, cost=6.899\n",
      ">epoch=30, lrate=0.000, cost=6.834\n",
      ">epoch=31, lrate=0.000, cost=6.770\n",
      ">epoch=32, lrate=0.000, cost=6.708\n",
      ">epoch=33, lrate=0.000, cost=6.648\n",
      ">epoch=34, lrate=0.000, cost=6.590\n",
      ">epoch=35, lrate=0.000, cost=6.533\n",
      ">epoch=36, lrate=0.000, cost=6.479\n",
      ">epoch=37, lrate=0.000, cost=6.425\n",
      ">epoch=38, lrate=0.000, cost=6.374\n",
      ">epoch=39, lrate=0.000, cost=6.324\n",
      ">epoch=40, lrate=0.000, cost=6.275\n",
      ">epoch=41, lrate=0.000, cost=6.228\n",
      ">epoch=42, lrate=0.000, cost=6.182\n",
      ">epoch=43, lrate=0.000, cost=6.137\n",
      ">epoch=44, lrate=0.000, cost=6.094\n",
      ">epoch=45, lrate=0.000, cost=6.052\n",
      ">epoch=46, lrate=0.000, cost=6.011\n",
      ">epoch=47, lrate=0.000, cost=5.971\n",
      ">epoch=48, lrate=0.000, cost=5.933\n",
      ">epoch=49, lrate=0.000, cost=5.895\n",
      "[{'weights': [0.0382102133547877, 0.8313089229358278, 0.7537235787034484], 'output': 1.0495884813197756, 'delta': -0.44234161683188594}, {'weights': [0.19307934467920768, 0.4704102072726401, 0.4386021871616383], 'output': 1.9208440057347378, 'delta': -0.0998469617657054}]\n",
      "[{'weights': [0.6263171675425927, 0.7477604198130632, 0.07803519661454099], 'output': 0.735565070321825, 'delta': -0.735565070321825}, {'weights': [0.03484393038134096, 0.8508760170937348, 0.4342764138185783], 'output': 0.47073696996805575, 'delta': 0.5292630300319443}]\n"
     ]
    }
   ],
   "source": [
    "train_network(network,dataset,learning_rate,epochs,n_outputs)\n",
    "for layer in network:\n",
    "\tprint(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch adalah ketika seluruh dataset sudah melalui proses training pada Neural Netwok sampai dikembalikan ke awal untuk sekali putaran, karena satu Epoch terlalu besar untuk dimasukkan (feeding) kedalam komputer maka dari itu kita perlu membaginya kedalam satuan kecil (batches). Jumlah Epoch tergantung pada data yang kita miliki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat dua kondisi stopping pada algoritma backpropagation ini, yaitu :\n",
    "\n",
    "   Error < Error maksimum\n",
    "\n",
    "Error adalah perbedaan yang terjadi antara ouput terhadap target yang diinginkan. Proses ANN akan berhenti jika bessarnya error yang terjadi telah bernilai lebih kecil dari nilai error maksimum yang telah ditetapkan. Besarnya nilai error dihitung dengan menggunakan fungsi error kuadratis.\n",
    "\n",
    "Fungsi eror tersebut merupakan bagian dari algoritma backpropagation yang menggunakan gradient descent yaitu dengan cara menuruni lembah permukaan error untuk mencapai nilai error yang convergen.\n",
    "\n",
    "   Epoch > Epoch maksimum\n",
    "\n",
    "Epoch adalah suatu langkah yang dilakukan dalam pembelajaran pada ANN. Jika besarnya epoch lebih besar dari besarnya epoch maksimum yang telah ditetapkan, maka proses pembelajaran akan berhenti.\n",
    "\n",
    "Kedua kondisi stopping di atas digunakan dengan logika OR. Jadi kondisi stopping adak terjadi jika besarnya Error < Error maksimum atau Epoch > Epoch maksimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/mrStasSmirnoff/Backpropagation_Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
