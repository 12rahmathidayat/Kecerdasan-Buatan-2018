{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KATA KASAR DI TWITTER MENGGUNAKAN ALGORITMA SUPPORT VECTOR MACHINE (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap berikutnya melakukan perintah untuk membuka dataset mentah. Dataset kemudian disimpan didalam variabel datar dengan kolom bernama Konten.\n",
    "\n",
    "Data bersumber dari tweet dengan hashtag ##TolakOmnibusLaw sebanyak 1000 data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREPROCESSING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**memanggil dataset**\n",
    "\n",
    "Tahap berikutnya melakukan perintah untuk membuka dataset mentah. Dataset kemudian disimpan didalam variabel datar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Yg dah follow dan mau di fb duluan tingglin je...\n",
      "1      How can dpr members who swear in the name of t...\n",
      "2      âPEJABATâ: \\nlahir dari RAKYAT, menjabat j...\n",
      "3      If you're listening to this, you are the resis...\n",
      "4      No Justice, no peace fuck the police \\n\\n#Tola...\n",
      "                             ...                        \n",
      "995    Memperingati satu tahun Jokowi yang langsung m...\n",
      "996    Simak fakta &amp; argumentasi Asfinawati, Dire...\n",
      "997    RUU yang telah disahkan oleh Sidang Paripurna,...\n",
      "998    Trus bergerak kawan... #MahasiswaBergerak\\n#To...\n",
      "999    Conto kerja keras tanpa keringat. https://t.co...\n",
      "Name: content, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "datar = pd.read_csv('twitter_hashtag.csv',encoding='latin1')\n",
    "\n",
    "print(datar['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap berikutnyaa yaitu pengambilan data yang ada dalam variabel content kemudian dilakukan perintah cleansing dan case folding. Cleansing dan case folding dilakukan pada  dengan menggunakan modul re dari regex dan  string untuk mendeteksi kata. Hal ini bertujuan untuk membersihkan data dari emoticon, url dan teks yang tidak jelas, kemudian disimpan kembali didalam variabel yang sama yaitu dan datar dan kolom content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [Yg, dah, follow, dan, mau, di, fb, duluan, ti...\n",
      "1      [How, can, dpr, members, who, swear, in, the, ...\n",
      "2      [lahir, dari, RAKYAT, menjabat, jadi, MOSITIDA...\n",
      "3      [If, you, listening, to, this, you, are, the, ...\n",
      "4      [No, Justice, no, peace, fuck, the, police, To...\n",
      "                             ...                        \n",
      "995    [Memperingati, satu, tahun, Jokowi, yang, lang...\n",
      "996    [Simak, fakta, amp, argumentasi, Asfinawati, D...\n",
      "997    [RUU, yang, telah, disahkan, oleh, Sidang, Par...\n",
      "998    [Trus, bergerak, kawan, MahasiswaBergerak, Tol...\n",
      "999    [Conto, kerja, keras, tanpa, keringat, https, ...\n",
      "Name: Word, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def identify_tokens(row):\n",
    "    description = row['content']\n",
    "    tokens = nltk.word_tokenize(str(description))\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "datar['Word'] = datar.apply(identify_tokens, axis=1)\n",
    "print(datar['Word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses Tokenize pada gambar diatas bertujuan untuk membuat data tepisah kata perkata. Library yang digunakan yaitu TweetTokenize yang berasal dari Library NLTK, kemudian disimpan lagi di variabel datar dengan nama kolom baru \n",
    "yaitu Word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya yaitu proses stemming. Proses stemming dilakukan menggunakan library Sastrawi. Library Sastrawi adalah library yang digunakan layaknya kamus, namun berisi kamus bahasa \n",
    "Indonesia berisi kata dasar. Jadi stemming dilakukan untuk merubah kalimat berimbuhan dan merubahnya menjadi kata dasar. Hasil dari tahap stemming ini disimpan ke dalam variabel yang sama yaitu datar dengan nama kolom baru, yaitu stemmed_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [yg, dah, follow, dan, mau, di, fb, duluan, ti...\n",
      "1      [how, can, dpr, members, who, swear, in, the, ...\n",
      "2      [lahir, dari, rakyat, jabat, jadi, mositidakdi...\n",
      "3      [if, you, listening, to, this, you, are, the, ...\n",
      "4      [no, justice, no, peace, fuck, the, police, to...\n",
      "                             ...                        \n",
      "995    [ingat, satu, tahun, jokowi, yang, langsung, t...\n",
      "996    [simak, fakta, amp, argumentasi, asfinawati, d...\n",
      "997    [ruu, yang, telah, sah, oleh, sidang, paripurn...\n",
      "998    [trus, gerak, kawan, mahasiswabergerak, tolako...\n",
      "999    [conto, kerja, keras, tanpa, keringat, https, ...\n",
      "Name: Stemmed_Word, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "factory = StemmerFactory()\n",
    "indostemming = factory.create_stemmer()\n",
    "\n",
    "def stem_list(row):\n",
    "    my_list = row['Word']\n",
    "    stemmed_list = [indostemming.stem(word) for word in my_list]\n",
    "    return(stemmed_list)\n",
    "\n",
    "datar['Stemmed_Word'] = datar.apply(stem_list, axis=1)\n",
    "print(datar['Stemmed_Word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahap selanjutnya menggunakan library NLTK.corpus dengan modul stopword untuk mendeteksi kosakata dan kalimat yang sering keluar pada sistem dan menghapus kata sambung. Kemudian data di simpan ke dalam variabel datar kembali namun \n",
    "dengan nama kolom baru dengan nama Stem_Meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [yg, dah, follow, fb, duluan, tingglin, jejak,...\n",
      "1      [dpr, members, swear, name, god, betray, peopl...\n",
      "2      [lahir, rakyat, jabat, mositidakdipercaya, cab...\n",
      "3      [listening, resistence, tolakomnibuslaw, menol...\n",
      "4      [justice, peace, fuck, police, tolakuuciptaker...\n",
      "                             ...                        \n",
      "995    [jokowi, langsung, jati, kolega, mositidakperc...\n",
      "996    [simak, fakta, amp, argumentasi, asfinawati, d...\n",
      "997    [ruu, sah, sidang, paripurna, sidang, paripurn...\n",
      "998    [trus, gerak, kawan, mahasiswabergerak, tolako...\n",
      "999        [conto, kerja, keras, keringat, https, https]\n",
      "Name: Stem_Meaningful, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "stops = set(stopwords.words(\"indonesian\"))\n",
    "stops_eng = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row[\"Stemmed_Word\"]\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    meaningful_words = [w for w in meaningful_words if not w in stops_eng]\n",
    "    meaningful_words = [stopword.remove(w) for w in meaningful_words]\n",
    "    return(meaningful_words)\n",
    "\n",
    "datar['Stem_Meaningful'] = datar.apply(remove_stops, axis=1)\n",
    "print(datar['Stem_Meaningful'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kemudian seluruh data yang sudah diproses didalam kolom baru pada variabel datar di gabungkan dengan data sebelumnya dengan nama variabel baru yaitu Processed melalui fungsi rejoin_word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      yg dah follow fb duluan tingglin jejak twit to...\n",
      "1      dpr members swear name god betray people makin...\n",
      "2      lahir rakyat jabat mositidakdipercaya cabutomn...\n",
      "3      listening resistence tolakomnibuslaw menolakpu...\n",
      "4      justice peace fuck police tolakuuciptakerja to...\n",
      "                             ...                        \n",
      "995    jokowi langsung jati kolega mositidakpercaya r...\n",
      "996    simak fakta amp argumentasi asfinawati direktu...\n",
      "997    ruu sah sidang paripurna sidang paripurna hapu...\n",
      "998    trus gerak kawan mahasiswabergerak tolakomnibu...\n",
      "999               conto kerja keras keringat https https\n",
      "Name: Processed, Length: 1000, dtype: object\n",
      "['tweetDate', 'content', 'twitterProfile', 'tweetUrl', 'timestamp', 'query', 'Word', 'Stemmed_Word', 'Stem_Meaningful', 'Processed']\n"
     ]
    }
   ],
   "source": [
    "def rejoin_words(row):\n",
    "    my_list = row['Stem_Meaningful']\n",
    "    joined_words = (\" \".join(my_list))\n",
    "    return joined_words\n",
    "\n",
    "datar['Processed'] = datar.apply(rejoin_words, axis=1)\n",
    "print(datar['Processed'])\n",
    "print(list(datar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data yang sudah digabungkan tadi akan memiliki banyak kolom yang tidak terpakai akibat penggabungan pada proses preprocessing data, oleh karena itu diperlukan proses penghapusan data oleh sistem menggunakan perintah drop seperti pada code dibawah sehingga menyisakan data dengan nama variabel label dan processed csv baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['content', 'Word', 'Stemmed_Word', 'Stem_Meaningful']\n",
    "datar = datar.drop(cols_to_drop, axis = 'columns')\n",
    "\n",
    "datar.to_csv('twitter_hashtag_baru.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODELLING**\n",
    "\n",
    "Setelah melewati tahap data preprocessing hingga menghasilkan data baru, selanjutnya tahap modelling. Modelling yang dilakukan juga telah dimplementasikan ke dalam kode Python dengan menggunakan algoritma Support Vector Machine (Support Vector Machine). Berikut merupakan tahapan pembuatan \n",
    "model dari Sentiment Analysis klasifikasi Kata Kasar dan Bukan Kata Kasar :\n",
    "\n",
    "sebelumnya kita melakukan perintah pembacaan data kembali pada data yang sudah selesai diproses melalui tahapan preprocessing data untuk persiapan proses modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label                                          Processed\n",
      "0    POSITIF  yg dah follow fb duluan tingglin jejak twit to...\n",
      "1    POSITIF  dpr members swear name god betray people makin...\n",
      "2    POSITIF  justice peace fuck police tolakuuciptakerja to...\n",
      "3    POSITIF  tuantigabelas fought system tolakomnibuslaw https\n",
      "4    POSITIF   come back mositidakpercaya tolakomnibuslaw https\n",
      "..       ...                                                ...\n",
      "994  POSITIF  semangat janganberhentibergerak tolakomnibusla...\n",
      "995  POSITIF  semangat janganberhentibergerak tolakomnibusla...\n",
      "996  POSITIF  semangat janganberhentibergerak tolakomnibusla...\n",
      "997  POSITIF  semangat janganberhentibergerak tolakomnibusla...\n",
      "998  POSITIF  semangat janganberhentibergerak tolakomnibusla...\n",
      "\n",
      "[999 rows x 2 columns]\n",
      "Index(['Label', 'Processed'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "datas = pd.read_csv('twitter_hashtag_kedua.csv')\n",
    "\n",
    "print(datas)\n",
    "print(datas.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows label\n",
      "NEGATIF    704\n",
      "POSITIF    295\n",
      "Name: Label, dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Number of rows label')\n",
    "print(datas['Label'].value_counts())\n",
    "print('')\n",
    "plt.figure()\n",
    "pd.value_counts(datas['Label']).plot.bar(title=\"jumlah data\")\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.show "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data dari setiap label :\n",
      "NEGATIF    704\n",
      "POSITIF    295\n",
      "Name: Label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>yg dah follow fb duluan tingglin jejak twit to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>dpr members swear name god betray people makin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>justice peace fuck police tolakuuciptakerja to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>tuantigabelas fought system tolakomnibuslaw https</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>come back mositidakpercaya tolakomnibuslaw https</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>omnibus law threatens indigenous peoples tradi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>poster said born people took office mositidakp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>goes jakarta elites reformasidikorupsi tolakom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>visited bogor palace still meet students peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>POSITIF</td>\n",
       "      <td>think movie scene photo indonesian student pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                          Processed\n",
       "0   POSITIF  yg dah follow fb duluan tingglin jejak twit to...\n",
       "1   POSITIF  dpr members swear name god betray people makin...\n",
       "2   POSITIF  justice peace fuck police tolakuuciptakerja to...\n",
       "3   POSITIF  tuantigabelas fought system tolakomnibuslaw https\n",
       "4   POSITIF   come back mositidakpercaya tolakomnibuslaw https\n",
       "5   POSITIF  omnibus law threatens indigenous peoples tradi...\n",
       "7   POSITIF  poster said born people took office mositidakp...\n",
       "8   POSITIF  goes jakarta elites reformasidikorupsi tolakom...\n",
       "9   POSITIF  visited bogor palace still meet students peopl...\n",
       "10  POSITIF  think movie scene photo indonesian student pro..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_top_data(top_n = 1000):\n",
    "    datas_positive = datas[datas['Label'] == 'POSITIF'].head(top_n)\n",
    "    datas_negative = datas[datas['Label'] == 'NEGATIF'].head(top_n)\n",
    "    datas_small = pd.concat([datas_positive,datas_negative])\n",
    "    return datas_small\n",
    "\n",
    "datas_small = get_top_data(top_n=855)\n",
    "\n",
    "print(\"Jumlah data dari setiap label :\")\n",
    "print((datas_small['Label'].value_counts()))\n",
    "datas_small.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gambar diatas menunjukkan proses sampling data. Sampling data dilakukan karena jumlah perbandingan data antara positif dan negatif tidak sama. Sampling data yang dilakukan sesuai dengan jumlah data negatif yaitu 704."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak data x_train :  899\n",
      "Banyak data x_test :  100\n",
      "Banyak data y_train :  899\n",
      "Banyak data y_test :  100\n",
      " \n",
      " \n",
      "Banyak data x_train :  849\n",
      "Banyak data x_test :  150\n",
      "Banyak data y_train :  849\n",
      "Banyak data y_test :  150\n",
      " \n",
      " \n",
      "Banyak data x_train :  799\n",
      "Banyak data x_test :  200\n",
      "Banyak data y_train :  799\n",
      "Banyak data y_test :  200\n",
      " \n",
      " \n",
      "Banyak data x_train :  749\n",
      "Banyak data x_test :  250\n",
      "Banyak data y_train :  749\n",
      "Banyak data y_test :  250\n",
      " \n",
      " \n",
      "Banyak data x_train :  699\n",
      "Banyak data x_test :  300\n",
      "Banyak data y_train :  699\n",
      "Banyak data y_test :  300\n",
      " \n",
      " \n",
      "Banyak data x_train :  649\n",
      "Banyak data x_test :  350\n",
      "Banyak data y_train :  649\n",
      "Banyak data y_test :  350\n",
      " \n",
      " \n",
      "Banyak data x_train :  599\n",
      "Banyak data x_test :  400\n",
      "Banyak data y_train :  599\n",
      "Banyak data y_test :  400\n",
      " \n",
      " \n",
      "Banyak data x_train :  549\n",
      "Banyak data x_test :  450\n",
      "Banyak data y_train :  549\n",
      "Banyak data y_test :  450\n",
      " \n",
      " \n",
      "Banyak data x_train :  499\n",
      "Banyak data x_test :  500\n",
      "Banyak data y_train :  499\n",
      "Banyak data y_test :  500\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "x = datas.Processed\n",
    "y = datas.Label\n",
    "\n",
    "x_small = datas_small.Processed\n",
    "y_small = datas_small.Label\n",
    "\n",
    "n_test_split = [0.1,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50]\n",
    "tvec = TfidfVectorizer(min_df=5, max_df =0.8, sublinear_tf = True, use_idf = True)\n",
    "\n",
    "for test in n_test_split:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_small, y_small, test_size = test)\n",
    "    \n",
    "    print('Banyak data x_train : ',len(x_train))\n",
    "    print('Banyak data x_test : ',len(x_test))\n",
    "    print('Banyak data y_train : ',len(y_train))\n",
    "    print('Banyak data y_test : ',len(y_test))\n",
    "    print(' ')\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gambar diatas menunjukkan proses split data secara looping sebanyak 9 kali agar menghasilkan akurasi yang optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langkah berikutnya menambahkan perintah untuk mengimport library dan method yang dibutuhkan untu proses modelling dengan algoritma Support Vector Machine. Selain itu menuliskan metode klasifikasi Support Vector Machine dengan kernel linear. Selain itu, dituliskan fungsi klasifikasi \n",
    "menggunakan Support Vector Machine dengan kernel linear yang disimpan dalam variabel classifier_linear. Hasil klasifikasi tersebut menghasilkan nilai akurasi dengan menggunakan fungsi accuracy_score dan juga hasil evaluasi akanditampung menggunakan classification report untuk menghasilkan data evaluasi seperti akurasi, presisi dan recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "klasifikasi 0.5\n",
      "Waktu training : 0.011943s; Waktu Test: 0.008973s\n",
      " \n",
      "Final Accuracy: 0.892\n",
      " \n",
      "Report klasifikasi               precision    recall  f1-score   support\n",
      "\n",
      "     NEGATIF       0.88      0.97      0.93       344\n",
      "     POSITIF       0.92      0.72      0.81       156\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.90      0.84      0.87       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n",
      " \n",
      "Confusion Matrix\n",
      "[[334  44]\n",
      " [ 10 112]]\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "train_vectors = tvec.fit_transform(x_train.values.astype('U'))\n",
    "test_vectors = tvec.transform(x_test.values.astype('U'))\n",
    "\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1 - t0\n",
    "time_linear_test = t2 - t1\n",
    "report = classification_report(y_test, prediction_linear)\n",
    "cfm = confusion_matrix(prediction_linear.astype(str),y_test.astype(str))\n",
    "\n",
    "#print\n",
    "print('klasifikasi %s' % (test))\n",
    "print(\"Waktu training : %fs; Waktu Test: %fs\" % (time_linear_train,time_linear_test))\n",
    "print(' ')\n",
    "print(\"Final Accuracy: %s\" % accuracy_score(y_test,prediction_linear))\n",
    "print(' ')\n",
    "print('Report klasifikasi', report)\n",
    "print(' ')\n",
    "print('Confusion Matrix')\n",
    "print(cfm)\n",
    "print(' ')\n",
    "print(' ')\n",
    "print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation k = 5\n",
      "Accuracy: 0.89 (+/- 0.03)\n",
      " \n",
      "hasil akurasi cross validation : kfold 1\n",
      "0.91\n",
      "\n",
      "hasil akurasi cross validation : kfold 2\n",
      "0.89\n",
      "\n",
      "hasil akurasi cross validation : kfold 3\n",
      "0.90\n",
      "\n",
      "hasil akurasi cross validation : kfold 4\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 5\n",
      "0.89\n",
      "\n",
      "\n",
      "\n",
      "cross validation k = 6\n",
      "Accuracy: 0.89 (+/- 0.06)\n",
      " \n",
      "hasil akurasi cross validation : kfold 1\n",
      "0.92\n",
      "\n",
      "hasil akurasi cross validation : kfold 2\n",
      "0.88\n",
      "\n",
      "hasil akurasi cross validation : kfold 3\n",
      "0.90\n",
      "\n",
      "hasil akurasi cross validation : kfold 4\n",
      "0.87\n",
      "\n",
      "hasil akurasi cross validation : kfold 5\n",
      "0.84\n",
      "\n",
      "hasil akurasi cross validation : kfold 6\n",
      "0.93\n",
      "\n",
      "\n",
      "\n",
      "cross validation k = 7\n",
      "Accuracy: 0.89 (+/- 0.08)\n",
      " \n",
      "hasil akurasi cross validation : kfold 1\n",
      "0.94\n",
      "\n",
      "hasil akurasi cross validation : kfold 2\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 3\n",
      "0.89\n",
      "\n",
      "hasil akurasi cross validation : kfold 4\n",
      "0.92\n",
      "\n",
      "hasil akurasi cross validation : kfold 5\n",
      "0.83\n",
      "\n",
      "hasil akurasi cross validation : kfold 6\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 7\n",
      "0.93\n",
      "\n",
      "\n",
      "\n",
      "cross validation k = 8\n",
      "Accuracy: 0.89 (+/- 0.05)\n",
      " \n",
      "hasil akurasi cross validation : kfold 1\n",
      "0.94\n",
      "\n",
      "hasil akurasi cross validation : kfold 2\n",
      "0.89\n",
      "\n",
      "hasil akurasi cross validation : kfold 3\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 4\n",
      "0.90\n",
      "\n",
      "hasil akurasi cross validation : kfold 5\n",
      "0.89\n",
      "\n",
      "hasil akurasi cross validation : kfold 6\n",
      "0.87\n",
      "\n",
      "hasil akurasi cross validation : kfold 7\n",
      "0.85\n",
      "\n",
      "hasil akurasi cross validation : kfold 8\n",
      "0.92\n",
      "\n",
      "\n",
      "\n",
      "cross validation k = 9\n",
      "Accuracy: 0.89 (+/- 0.07)\n",
      " \n",
      "hasil akurasi cross validation : kfold 1\n",
      "0.93\n",
      "\n",
      "hasil akurasi cross validation : kfold 2\n",
      "0.91\n",
      "\n",
      "hasil akurasi cross validation : kfold 3\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 4\n",
      "0.89\n",
      "\n",
      "hasil akurasi cross validation : kfold 5\n",
      "0.95\n",
      "\n",
      "hasil akurasi cross validation : kfold 6\n",
      "0.82\n",
      "\n",
      "hasil akurasi cross validation : kfold 7\n",
      "0.87\n",
      "\n",
      "hasil akurasi cross validation : kfold 8\n",
      "0.87\n",
      "\n",
      "hasil akurasi cross validation : kfold 9\n",
      "0.91\n",
      "\n",
      "\n",
      "\n",
      "cross validation k = 10\n",
      "Accuracy: 0.89 (+/- 0.05)\n",
      " \n",
      "hasil akurasi cross validation : kfold 1\n",
      "0.94\n",
      "\n",
      "hasil akurasi cross validation : kfold 2\n",
      "0.88\n",
      "\n",
      "hasil akurasi cross validation : kfold 3\n",
      "0.88\n",
      "\n",
      "hasil akurasi cross validation : kfold 4\n",
      "0.90\n",
      "\n",
      "hasil akurasi cross validation : kfold 5\n",
      "0.90\n",
      "\n",
      "hasil akurasi cross validation : kfold 6\n",
      "0.90\n",
      "\n",
      "hasil akurasi cross validation : kfold 7\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 8\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 9\n",
      "0.86\n",
      "\n",
      "hasil akurasi cross validation : kfold 10\n",
      "0.92\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_val = [5,6,7,8,9,10]\n",
    "for n in n_val:\n",
    "    kfold = KFold(n_splits=n)\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    scores = cross_val_score(clf, test_vectors, y_test, cv=kfold)\n",
    "    \n",
    "    print(\"cross validation k = %s\" %(n))\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "    print(' ')\n",
    "    index = 0\n",
    "    for i in scores:\n",
    "        index += 1\n",
    "        print('hasil akurasi cross validation : kfold', index)\n",
    "        print(\"%0.2f\" % i)\n",
    "        print('')\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pengujian dilakukan untuk mengklasifikasi data ke arah negatif (tidak mengandung kata kasar) atau positif (mengandung kata kasar) dan menghasilkan 704 data negatif dan 295 data positif dari total jumlah 1000 data dari hashtag #TolakOmnibusLaw dan dilakukan sampling kembali setelah di \n",
    "lakukan tahap preprocessing data, dan dimasukkan ke dalam proses modelling dan menjadi data training dan data testing. Pengujian Sentiment Analysis akan menguji model yang telah dibuat untuk mengeluarkan akurasi dengan evaluasi melalui metode split data dan k-fold cross validation. Tujuannya adalah untuk mengevaluasi model yang telah dibuat dan \n",
    "memperoleh akurasi yang optimal dari kedua metode yang dipakai."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
